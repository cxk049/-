{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Device: mps:0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 创建一个大的张量来模拟GPU操作\n",
    "x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32, device='mps')\n",
    "print(\"MPS Device:\", x.device)\n",
    "\n",
    "# 使用 MPS 设备\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 加载预训练的 ResNet-18 模型\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 修改模型的最后一层以适应新的类别数\n",
    "num_classes = 200\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# 将模型移动到 MPS 设备\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 设置优化器，微调模型的不同部分\n",
    "optimizer = optim.SGD([\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-2},\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-3},\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer2.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer1.parameters(), 'lr': 1e-6},\n",
    "], momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed!\n"
     ]
    }
   ],
   "source": [
    "# 设置数据集路径\n",
    "data_dir = '/Users/fuchenxu/Desktop/computer vision/midterm_project/task_1/CUB_200_2011'\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "split_file = os.path.join(data_dir, 'train_test_split.txt')\n",
    "images_file = os.path.join(data_dir, 'images.txt')\n",
    "\n",
    "# 创建train和val目录\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# 读取images.txt文件\n",
    "image_paths = {}\n",
    "with open(images_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        img_id, img_path = line.strip().split()\n",
    "        image_paths[int(img_id)] = img_path\n",
    "\n",
    "# 读取分割文件并进行数据分割\n",
    "with open(split_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        img_id, is_train = line.strip().split()\n",
    "        img_id = int(img_id)\n",
    "        is_train = int(is_train)\n",
    "\n",
    "        # 获取图像文件路径\n",
    "        img_file = os.path.join(images_dir, image_paths[img_id])\n",
    "\n",
    "        # 确定目标路径\n",
    "        if is_train == 1:\n",
    "            target_dir = train_dir\n",
    "        else:\n",
    "            target_dir = val_dir\n",
    "\n",
    "        # 确保目标子目录存在\n",
    "        class_name = os.path.basename(os.path.dirname(img_file))\n",
    "        target_class_dir = os.path.join(target_dir, class_name)\n",
    "        os.makedirs(target_class_dir, exist_ok=True)\n",
    "\n",
    "        # 复制图像到目标目录\n",
    "        shutil.copy(img_file, target_class_dir)\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 5994\n",
      "Validation dataset size: 5794\n",
      "Class names: ['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '010.Red_winged_Blackbird', '011.Rusty_Blackbird', '012.Yellow_headed_Blackbird', '013.Bobolink', '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting', '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird', '020.Yellow_breasted_Chat', '021.Eastern_Towhee', '022.Chuck_will_Widow', '023.Brandt_Cormorant', '024.Red_faced_Cormorant', '025.Pelagic_Cormorant', '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper', '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo', '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo', '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch', '036.Northern_Flicker', '037.Acadian_Flycatcher', '038.Great_Crested_Flycatcher', '039.Least_Flycatcher', '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher', '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher', '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall', '047.American_Goldfinch', '048.European_Goldfinch', '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe', '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak', '055.Evening_Grosbeak', '056.Pine_Grosbeak', '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot', '059.California_Gull', '060.Glaucous_winged_Gull', '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull', '064.Ring_billed_Gull', '065.Slaty_backed_Gull', '066.Western_Gull', '067.Anna_Hummingbird', '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird', '070.Green_Violetear', '071.Long_tailed_Jaeger', '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay', '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird', '078.Gray_Kingbird', '079.Belted_Kingfisher', '080.Green_Kingfisher', '081.Pied_Kingfisher', '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher', '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon', '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser', '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk', '093.Clark_Nutcracker', '094.White_breasted_Nuthatch', '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole', '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican', '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis', '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin', '107.Common_Raven', '108.White_necked_Raven', '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike', '112.Great_Grey_Shrike', '113.Baird_Sparrow', '114.Black_throated_Sparrow', '115.Brewer_Sparrow', '116.Chipping_Sparrow', '117.Clay_colored_Sparrow', '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow', '121.Grasshopper_Sparrow', '122.Harris_Sparrow', '123.Henslow_Sparrow', '124.Le_Conte_Sparrow', '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow', '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow', '130.Tree_Sparrow', '131.Vesper_Sparrow', '132.White_crowned_Sparrow', '133.White_throated_Sparrow', '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow', '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager', '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern', '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern', '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee', '149.Brown_Thrasher', '150.Sage_Thrasher', '151.Black_capped_Vireo', '152.Blue_headed_Vireo', '153.Philadelphia_Vireo', '154.Red_eyed_Vireo', '155.Warbling_Vireo', '156.White_eyed_Vireo', '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler', '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler', '161.Blue_winged_Warbler', '162.Canada_Warbler', '163.Cape_May_Warbler', '164.Cerulean_Warbler', '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler', '167.Hooded_Warbler', '168.Kentucky_Warbler', '169.Magnolia_Warbler', '170.Mourning_Warbler', '171.Myrtle_Warbler', '172.Nashville_Warbler', '173.Orange_crowned_Warbler', '174.Palm_Warbler', '175.Pine_Warbler', '176.Prairie_Warbler', '177.Prothonotary_Warbler', '178.Swainson_Warbler', '179.Tennessee_Warbler', '180.Wilson_Warbler', '181.Worm_eating_Warbler', '182.Yellow_Warbler', '183.Northern_Waterthrush', '184.Louisiana_Waterthrush', '185.Bohemian_Waxwing', '186.Cedar_Waxwing', '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker', '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker', '191.Red_headed_Woodpecker', '192.Downy_Woodpecker', '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren', '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren', '199.Winter_Wren', '200.Common_Yellowthroat']\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义数据预处理步骤\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),  # 随机裁剪并调整为224x224大小\n",
    "        transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "        transforms.ToTensor(),  # 转换为Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 标准化处理\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),  # 调整图像大小\n",
    "        transforms.CenterCrop(224),  # 中心裁剪为224x224大小\n",
    "        transforms.ToTensor(),  # 转换为Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 标准化处理\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 设置数据集路径\n",
    "data_dir = 'CUB_200_2011'  \n",
    "\n",
    "# 检查train和val目录是否存在\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "if not os.path.exists(train_dir):\n",
    "    raise FileNotFoundError(f'Training directory not found: {train_dir}')\n",
    "if not os.path.exists(val_dir):\n",
    "    raise FileNotFoundError(f'Validation directory not found: {val_dir}')\n",
    "\n",
    "# 3. 创建训练和验证的数据集\n",
    "image_datasets = {\n",
    "    'train': ImageFolder(root=train_dir, transform=data_transforms['train']),\n",
    "    'val': ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "# 4. 创建数据加载器\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "# 5. 获取数据集大小\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(f\"Training dataset size: {dataset_sizes['train']}\")\n",
    "print(f\"Validation dataset size: {dataset_sizes['val']}\")\n",
    "\n",
    "# 6. 获取类别名称\n",
    "class_names = image_datasets['train'].classes\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs=20):\n",
    "    best_model_wts = None\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # 初始化TensorBoard SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # 记录损失和准确率到TensorBoard\n",
    "            writer.add_scalar(f'{phase}_Loss', epoch_loss, epoch)\n",
    "            writer.add_scalar(f'{phase}_Accuracy', epoch_acc, epoch)\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # 关闭TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.16.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"/Users/fuchenxu/Desktop/computer vision/midterm_project/task_1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'best_model_20240520-093549.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 获取当前时间并格式化\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filename = f'best_model_{current_time}.pth'\n",
    "\n",
    "# 保存模型权重\n",
    "torch.save(model.state_dict(), filename)\n",
    "print(f\"Model saved as '{filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 4.2920 Acc: 0.1253\n",
      "val Loss: 2.5077 Acc: 0.3771\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 2.4599 Acc: 0.4164\n",
      "val Loss: 1.7314 Acc: 0.5352\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.8941 Acc: 0.5255\n",
      "val Loss: 1.4016 Acc: 0.6170\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.6167 Acc: 0.5874\n",
      "val Loss: 1.2796 Acc: 0.6486\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.4183 Acc: 0.6428\n",
      "val Loss: 1.2165 Acc: 0.6609\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.3027 Acc: 0.6678\n",
      "val Loss: 1.1157 Acc: 0.6873\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.1491 Acc: 0.7032\n",
      "val Loss: 1.1183 Acc: 0.6873\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.1370 Acc: 0.7119\n",
      "val Loss: 1.0885 Acc: 0.6974\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.0257 Acc: 0.7426\n",
      "val Loss: 1.0672 Acc: 0.7016\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.9851 Acc: 0.7466\n",
      "val Loss: 1.0539 Acc: 0.7113\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.9204 Acc: 0.7629\n",
      "val Loss: 1.0484 Acc: 0.7083\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.8982 Acc: 0.7688\n",
      "val Loss: 1.0504 Acc: 0.7175\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.8458 Acc: 0.7821\n",
      "val Loss: 1.0313 Acc: 0.7156\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.8116 Acc: 0.7943\n",
      "val Loss: 1.0684 Acc: 0.7133\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.7681 Acc: 0.8055\n",
      "val Loss: 1.0525 Acc: 0.7161\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.7670 Acc: 0.8043\n",
      "val Loss: 1.0397 Acc: 0.7228\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.7232 Acc: 0.8190\n",
      "val Loss: 1.0599 Acc: 0.7169\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.6883 Acc: 0.8297\n",
      "val Loss: 1.0264 Acc: 0.7271\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.6653 Acc: 0.8358\n",
      "val Loss: 1.0544 Acc: 0.7185\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.6812 Acc: 0.8278\n",
      "val Loss: 1.0698 Acc: 0.7209\n",
      "Best val Acc: 0.7271\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 20\n",
    "model = train_model(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model(num_classes, use_pretrained=True):\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def set_optimizer(model, learning_rate):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    return optimizer\n",
    "\n",
    "# 训练模型\n",
    "num_classes = 200\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-2  # 使用单一学习率进行简化\n",
    "\n",
    "model = initialize_model(num_classes=num_classes, use_pretrained=False).to(device)\n",
    "optimizer = set_optimizer(model, learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_from_random_initialization = train_model(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model2(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs=20):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # 初始化TensorBoard SummaryWriter\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # 设置学习率衰减\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 每个阶段的训练和验证\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 设置模型为训练模式\n",
    "            else:\n",
    "                model.eval()   # 设置模型为评估模式\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 迭代数据\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 清零梯度\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 前向传播\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # 反向传播+优化只在训练阶段\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # 记录损失和准确率到TensorBoard\n",
    "            writer.add_scalar(f'{phase}_Loss', epoch_loss, epoch)\n",
    "            writer.add_scalar(f'{phase}_Accuracy', epoch_acc, epoch)\n",
    "\n",
    "            # 深拷贝模型\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # 关闭TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 4.2656 Acc: 0.1376\n",
      "val Loss: 2.5544 Acc: 0.3769\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 2.4822 Acc: 0.4101\n",
      "val Loss: 1.7429 Acc: 0.5259\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 1.8937 Acc: 0.5232\n",
      "val Loss: 1.3962 Acc: 0.6206\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.6329 Acc: 0.5889\n",
      "val Loss: 1.2781 Acc: 0.6422\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.4293 Acc: 0.6400\n",
      "val Loss: 1.1714 Acc: 0.6678\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 1.1833 Acc: 0.7187\n",
      "val Loss: 1.0351 Acc: 0.7190\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 1.1190 Acc: 0.7389\n",
      "val Loss: 1.0330 Acc: 0.7201\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 1.1579 Acc: 0.7326\n",
      "val Loss: 1.0201 Acc: 0.7209\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 1.1176 Acc: 0.7372\n",
      "val Loss: 1.0157 Acc: 0.7201\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 1.1017 Acc: 0.7431\n",
      "val Loss: 1.0172 Acc: 0.7228\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 1.1061 Acc: 0.7471\n",
      "val Loss: 1.0143 Acc: 0.7232\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 1.0790 Acc: 0.7526\n",
      "val Loss: 1.0120 Acc: 0.7237\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 1.0776 Acc: 0.7559\n",
      "val Loss: 1.0060 Acc: 0.7270\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 1.0832 Acc: 0.7508\n",
      "val Loss: 1.0056 Acc: 0.7237\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 1.0610 Acc: 0.7568\n",
      "val Loss: 1.0115 Acc: 0.7242\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 1.0987 Acc: 0.7484\n",
      "val Loss: 1.0147 Acc: 0.7216\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 1.0691 Acc: 0.7544\n",
      "val Loss: 1.0106 Acc: 0.7245\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 1.0859 Acc: 0.7479\n",
      "val Loss: 1.0096 Acc: 0.7239\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 1.0600 Acc: 0.7541\n",
      "val Loss: 1.0097 Acc: 0.7207\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 1.0809 Acc: 0.7452\n",
      "val Loss: 1.0069 Acc: 0.7258\n",
      "Best val Acc: 0.7270\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 20\n",
    "model = train_model2(model, criterion, optimizer, dataloaders, dataset_sizes, device, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
